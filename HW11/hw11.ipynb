{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification\n",
    "\n",
    "Using neural networks, we will classify pieces of text according to the emotions they invoke. The dataset can be obtained [here](https://www.kaggle.com/datasets/ishantjuyal/emotions-in-text). The data consists of two columns: the first column is the text, and the second column is a manually annotated emotion associated with each text. We will make models that take the text as input and predict what category of emotion that text should be associated with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll read in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/Emotion_final.csv')\n",
    "df.dropna() # drop rows with missing data\n",
    "df['EmotionAsFactor'] = pd.factorize(df.Emotion)[0] # convert Emotion column to ints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make a frequency graph of all the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x22d8b39ac50>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sb\n",
    "\n",
    "sb.catplot(x='Emotion', kind='count', data=df[['Emotion']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is slightly unbalanced in favor of happiness and sadness. Fortunately, the other categories are represented enough that any model cannot achieve an accuracy of over 70% by exclusively guessing those two categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "First, we will divide the data into train and test sets. Every model we create will use the same label data, so we also go ahead and encode the target columns in both sets. The input data will have to be separately encoded depending on the type of model being created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# setting random seeds\n",
    "tf.keras.utils.set_random_seed(1234)\n",
    "np.random.seed(1234)\n",
    "\n",
    "# train/test split\n",
    "i = np.random.rand(len(df)) < 0.8\n",
    "train = df[i]\n",
    "test = df[~i]\n",
    "\n",
    "num_classes = df.EmotionAsFactor.nunique()\n",
    "vocab_size = 20000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(train.Text)\n",
    "\n",
    "# encode label column\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train.EmotionAsFactor)\n",
    "y_train = encoder.transform(train.EmotionAsFactor)\n",
    "y_test = encoder.transform(test.EmotionAsFactor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Neural Network\n",
    "\n",
    "We will make a dense sequential neural network and evaluate it on the test data. For this model, the input data is transformed to matrices according to the tf-idf frequency measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "108/108 [==============================] - 3s 20ms/step - loss: 1.6612 - sparse_categorical_accuracy: 0.3303 - val_loss: 1.6081 - val_sparse_categorical_accuracy: 0.2958\n",
      "Epoch 2/30\n",
      "108/108 [==============================] - 2s 16ms/step - loss: 1.5411 - sparse_categorical_accuracy: 0.3395 - val_loss: 1.5832 - val_sparse_categorical_accuracy: 0.2958\n",
      "Epoch 3/30\n",
      "108/108 [==============================] - 2s 16ms/step - loss: 1.4608 - sparse_categorical_accuracy: 0.3395 - val_loss: 1.5434 - val_sparse_categorical_accuracy: 0.2964\n",
      "Epoch 4/30\n",
      "108/108 [==============================] - 2s 16ms/step - loss: 1.3499 - sparse_categorical_accuracy: 0.3461 - val_loss: 1.4970 - val_sparse_categorical_accuracy: 0.3074\n",
      "Epoch 5/30\n",
      "108/108 [==============================] - 2s 16ms/step - loss: 1.2130 - sparse_categorical_accuracy: 0.4070 - val_loss: 1.4270 - val_sparse_categorical_accuracy: 0.4006\n",
      "Epoch 6/30\n",
      "108/108 [==============================] - 2s 15ms/step - loss: 1.0520 - sparse_categorical_accuracy: 0.5453 - val_loss: 1.3698 - val_sparse_categorical_accuracy: 0.5272\n",
      "Epoch 7/30\n",
      "108/108 [==============================] - 2s 16ms/step - loss: 0.8561 - sparse_categorical_accuracy: 0.6945 - val_loss: 1.2697 - val_sparse_categorical_accuracy: 0.5683\n",
      "Epoch 8/30\n",
      "108/108 [==============================] - 2s 16ms/step - loss: 0.6502 - sparse_categorical_accuracy: 0.7673 - val_loss: 1.2006 - val_sparse_categorical_accuracy: 0.6274\n",
      "Epoch 9/30\n",
      "108/108 [==============================] - 2s 15ms/step - loss: 0.4981 - sparse_categorical_accuracy: 0.8535 - val_loss: 1.1787 - val_sparse_categorical_accuracy: 0.6687\n",
      "Epoch 10/30\n",
      "108/108 [==============================] - 2s 16ms/step - loss: 0.3860 - sparse_categorical_accuracy: 0.8975 - val_loss: 1.1717 - val_sparse_categorical_accuracy: 0.6801\n",
      "Epoch 11/30\n",
      "108/108 [==============================] - 2s 16ms/step - loss: 0.2987 - sparse_categorical_accuracy: 0.9174 - val_loss: 1.1794 - val_sparse_categorical_accuracy: 0.6891\n",
      "Epoch 12/30\n",
      "108/108 [==============================] - 2s 23ms/step - loss: 0.2356 - sparse_categorical_accuracy: 0.9253 - val_loss: 1.1846 - val_sparse_categorical_accuracy: 0.6964\n",
      "Epoch 13/30\n",
      "108/108 [==============================] - 2s 19ms/step - loss: 0.1918 - sparse_categorical_accuracy: 0.9343 - val_loss: 1.1989 - val_sparse_categorical_accuracy: 0.6961\n",
      "Epoch 14/30\n",
      "108/108 [==============================] - 2s 16ms/step - loss: 0.1592 - sparse_categorical_accuracy: 0.9515 - val_loss: 1.2229 - val_sparse_categorical_accuracy: 0.7031\n",
      "Epoch 15/30\n",
      "108/108 [==============================] - 2s 17ms/step - loss: 0.1332 - sparse_categorical_accuracy: 0.9650 - val_loss: 1.2542 - val_sparse_categorical_accuracy: 0.7112\n",
      "Epoch 16/30\n",
      "108/108 [==============================] - 2s 16ms/step - loss: 0.1126 - sparse_categorical_accuracy: 0.9720 - val_loss: 1.2978 - val_sparse_categorical_accuracy: 0.7100\n",
      "Epoch 17/30\n",
      "108/108 [==============================] - 2s 17ms/step - loss: 0.0965 - sparse_categorical_accuracy: 0.9762 - val_loss: 1.2987 - val_sparse_categorical_accuracy: 0.7095\n",
      "Epoch 18/30\n",
      "108/108 [==============================] - 2s 17ms/step - loss: 0.0836 - sparse_categorical_accuracy: 0.9792 - val_loss: 1.3139 - val_sparse_categorical_accuracy: 0.7135\n",
      "Epoch 19/30\n",
      "108/108 [==============================] - 2s 17ms/step - loss: 0.0737 - sparse_categorical_accuracy: 0.9810 - val_loss: 1.3397 - val_sparse_categorical_accuracy: 0.7138\n",
      "Epoch 20/30\n",
      "108/108 [==============================] - 2s 16ms/step - loss: 0.0656 - sparse_categorical_accuracy: 0.9832 - val_loss: 1.3787 - val_sparse_categorical_accuracy: 0.7112\n",
      "Epoch 21/30\n",
      "108/108 [==============================] - 2s 17ms/step - loss: 0.0590 - sparse_categorical_accuracy: 0.9849 - val_loss: 1.4459 - val_sparse_categorical_accuracy: 0.7112\n",
      "Epoch 22/30\n",
      "108/108 [==============================] - 2s 18ms/step - loss: 0.0532 - sparse_categorical_accuracy: 0.9856 - val_loss: 1.4692 - val_sparse_categorical_accuracy: 0.7135\n",
      "Epoch 23/30\n",
      "108/108 [==============================] - 2s 18ms/step - loss: 0.0489 - sparse_categorical_accuracy: 0.9861 - val_loss: 1.5086 - val_sparse_categorical_accuracy: 0.7135\n",
      "Epoch 24/30\n",
      "108/108 [==============================] - 2s 16ms/step - loss: 0.0441 - sparse_categorical_accuracy: 0.9874 - val_loss: 1.5271 - val_sparse_categorical_accuracy: 0.7045\n",
      "Epoch 25/30\n",
      "108/108 [==============================] - 2s 16ms/step - loss: 0.0403 - sparse_categorical_accuracy: 0.9885 - val_loss: 1.5533 - val_sparse_categorical_accuracy: 0.7103\n",
      "Epoch 26/30\n",
      "108/108 [==============================] - 2s 15ms/step - loss: 0.0371 - sparse_categorical_accuracy: 0.9895 - val_loss: 1.5524 - val_sparse_categorical_accuracy: 0.7068\n",
      "Epoch 27/30\n",
      "108/108 [==============================] - 2s 15ms/step - loss: 0.0339 - sparse_categorical_accuracy: 0.9902 - val_loss: 1.6183 - val_sparse_categorical_accuracy: 0.7048\n",
      "Epoch 28/30\n",
      "108/108 [==============================] - 2s 15ms/step - loss: 0.0313 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.6403 - val_sparse_categorical_accuracy: 0.7086\n",
      "Epoch 29/30\n",
      "108/108 [==============================] - 2s 15ms/step - loss: 0.0288 - sparse_categorical_accuracy: 0.9909 - val_loss: 1.6565 - val_sparse_categorical_accuracy: 0.7080\n",
      "Epoch 30/30\n",
      "108/108 [==============================] - 2s 15ms/step - loss: 0.0270 - sparse_categorical_accuracy: 0.9915 - val_loss: 1.7001 - val_sparse_categorical_accuracy: 0.7039\n",
      "134/134 [==============================] - 1s 4ms/step\n",
      "\n",
      "accuracy:  0.8332555970149254\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "batch_size = 128\n",
    "num_epochs = 30\n",
    "\n",
    "# transform input\n",
    "x_train = tokenizer.texts_to_matrix(train.Text, mode='tfidf')\n",
    "x_test = tokenizer.texts_to_matrix(test.Text, mode='tfidf')\n",
    "\n",
    "# define model topology\n",
    "model_seq = models.Sequential()\n",
    "model_seq.add(layers.Dense(16, kernel_initializer='normal', activation='sigmoid'))\n",
    "model_seq.add(layers.Dense(16, kernel_initializer='normal', activation='relu'))\n",
    "model_seq.add(layers.Dense(num_classes, input_dim=vocab_size, kernel_initializer='normal', activation='softmax'))\n",
    "\n",
    "# train\n",
    "model_seq.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='rmsprop',\n",
    "    metrics=['sparse_categorical_accuracy']\n",
    ")\n",
    "\n",
    "# apply to test data\n",
    "model_seq.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=num_epochs,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "pred_seq = model_seq.predict(x_test) # get predictions as label probabilities\n",
    "pred_seq = np.argmax(pred_seq, axis=1) # get most likely label from probabilities\n",
    "\n",
    "print('\\naccuracy: ', accuracy_score(y_test, pred_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a fairly good accuracy of just over 80% with this simple network. This is fairly promising.\n",
    "\n",
    "Also note that we used the softmax activation function in the last layer of the model, and the number of output nodes is equal to the number of classes. This has to be done to make the network topology compatible with multiclass classification, and the same will be done for all future models. For similar reasons, 'sparse_categorical_crossentropy' must be the loss function used during each model's compilation step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network\n",
    "\n",
    "We will now create a convolutional network and evaluate it. For this model, the inputs are transformed to sequences with an arbitrary maximum length of 500. The number of epochs has been reduced in the interest of training the model more quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "108/108 [==============================] - 31s 260ms/step - loss: 1.5820 - sparse_categorical_accuracy: 0.3342 - val_loss: 1.6207 - val_sparse_categorical_accuracy: 0.3071\n",
      "Epoch 2/20\n",
      "108/108 [==============================] - 31s 287ms/step - loss: 1.5690 - sparse_categorical_accuracy: 0.3438 - val_loss: 1.6353 - val_sparse_categorical_accuracy: 0.2958\n",
      "Epoch 3/20\n",
      "108/108 [==============================] - 34s 311ms/step - loss: 1.5508 - sparse_categorical_accuracy: 0.3613 - val_loss: 1.6015 - val_sparse_categorical_accuracy: 0.3493\n",
      "Epoch 4/20\n",
      "108/108 [==============================] - 34s 315ms/step - loss: 1.4283 - sparse_categorical_accuracy: 0.4512 - val_loss: 1.4726 - val_sparse_categorical_accuracy: 0.4224\n",
      "Epoch 5/20\n",
      "108/108 [==============================] - 33s 304ms/step - loss: 1.1790 - sparse_categorical_accuracy: 0.5614 - val_loss: 1.3101 - val_sparse_categorical_accuracy: 0.5013\n",
      "Epoch 6/20\n",
      "108/108 [==============================] - 28s 262ms/step - loss: 0.9636 - sparse_categorical_accuracy: 0.6402 - val_loss: 1.2338 - val_sparse_categorical_accuracy: 0.5418\n",
      "Epoch 7/20\n",
      "108/108 [==============================] - 28s 256ms/step - loss: 0.7839 - sparse_categorical_accuracy: 0.7085 - val_loss: 1.1907 - val_sparse_categorical_accuracy: 0.5552\n",
      "Epoch 8/20\n",
      "108/108 [==============================] - 28s 262ms/step - loss: 0.6590 - sparse_categorical_accuracy: 0.7592 - val_loss: 1.1912 - val_sparse_categorical_accuracy: 0.5665\n",
      "Epoch 9/20\n",
      "108/108 [==============================] - 28s 261ms/step - loss: 0.5706 - sparse_categorical_accuracy: 0.7958 - val_loss: 1.2532 - val_sparse_categorical_accuracy: 0.5738\n",
      "Epoch 10/20\n",
      "108/108 [==============================] - 29s 265ms/step - loss: 0.5071 - sparse_categorical_accuracy: 0.8195 - val_loss: 1.2710 - val_sparse_categorical_accuracy: 0.5825\n",
      "Epoch 11/20\n",
      "108/108 [==============================] - 29s 273ms/step - loss: 0.4557 - sparse_categorical_accuracy: 0.8402 - val_loss: 1.3253 - val_sparse_categorical_accuracy: 0.5825\n",
      "Epoch 12/20\n",
      "108/108 [==============================] - 37s 346ms/step - loss: 0.4117 - sparse_categorical_accuracy: 0.8540 - val_loss: 1.3622 - val_sparse_categorical_accuracy: 0.5805\n",
      "Epoch 13/20\n",
      "108/108 [==============================] - 38s 352ms/step - loss: 0.3715 - sparse_categorical_accuracy: 0.8668 - val_loss: 1.4371 - val_sparse_categorical_accuracy: 0.5793\n",
      "Epoch 14/20\n",
      "108/108 [==============================] - 38s 348ms/step - loss: 0.3420 - sparse_categorical_accuracy: 0.8749 - val_loss: 1.6312 - val_sparse_categorical_accuracy: 0.5776\n",
      "Epoch 15/20\n",
      "108/108 [==============================] - 38s 349ms/step - loss: 0.3153 - sparse_categorical_accuracy: 0.8852 - val_loss: 1.6846 - val_sparse_categorical_accuracy: 0.5718\n",
      "Epoch 16/20\n",
      "108/108 [==============================] - 38s 349ms/step - loss: 0.2939 - sparse_categorical_accuracy: 0.8907 - val_loss: 1.6600 - val_sparse_categorical_accuracy: 0.5726\n",
      "Epoch 17/20\n",
      "108/108 [==============================] - 38s 347ms/step - loss: 0.2750 - sparse_categorical_accuracy: 0.8964 - val_loss: 1.7408 - val_sparse_categorical_accuracy: 0.5761\n",
      "Epoch 18/20\n",
      "108/108 [==============================] - 37s 347ms/step - loss: 0.2585 - sparse_categorical_accuracy: 0.9016 - val_loss: 1.8199 - val_sparse_categorical_accuracy: 0.5654\n",
      "Epoch 19/20\n",
      "108/108 [==============================] - 38s 351ms/step - loss: 0.2460 - sparse_categorical_accuracy: 0.9055 - val_loss: 1.8998 - val_sparse_categorical_accuracy: 0.5575\n",
      "Epoch 20/20\n",
      "108/108 [==============================] - 35s 327ms/step - loss: 0.2356 - sparse_categorical_accuracy: 0.9080 - val_loss: 1.9785 - val_sparse_categorical_accuracy: 0.5706\n",
      "134/134 [==============================] - 2s 16ms/step\n",
      "\n",
      "accuracy:  0.6450559701492538\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_len = 500\n",
    "batch_size = 128\n",
    "num_epochs = 20\n",
    "\n",
    "x_train = pad_sequences(tokenizer.texts_to_sequences(train.Text), maxlen=max_len)\n",
    "x_test  = pad_sequences(tokenizer.texts_to_sequences(test.Text), maxlen=max_len)\n",
    "\n",
    "model_cnn = models.Sequential()\n",
    "model_cnn.add(layers.Embedding(vocab_size, 128, input_length=max_len)) \n",
    "model_cnn.add(layers.Conv1D(32, 7, activation='relu')) \n",
    "model_cnn.add(layers.MaxPooling1D(5)) \n",
    "model_cnn.add(layers.Conv1D(32, 7, activation='relu')) \n",
    "model_cnn.add(layers.GlobalMaxPooling1D())\n",
    "model_cnn.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model_cnn.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='rmsprop',\n",
    "    metrics=['sparse_categorical_accuracy']\n",
    ")\n",
    " \n",
    "model_cnn.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=num_epochs,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "pred_cnn = model_cnn.predict(x_test)\n",
    "pred_cnn = np.argmax(pred_cnn, axis=1)\n",
    "\n",
    "print('\\naccuracy: ', accuracy_score(y_test, pred_cnn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of this CNN is unfortunately much lower than our original sequential network. The accuracy seems to have been increasing at a steady rate between all the epochs, so it is likely that more epochs or a greater batch size would result in a more accurate model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "\n",
    "Now we will try different embedding schemes to try to improve our results. For these, we will not have to transform our existing sequence representations of the input data.\n",
    "\n",
    "First, we'll try using pretrained embeddings from GloVe. Here we will use the 100 dimension embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "\n",
    "embeddings_index = {}\n",
    "with open('data/glove.6B/glove.6B.100d.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "108/108 [==============================] - 35s 313ms/step - loss: 1.5724 - sparse_categorical_accuracy: 0.3604 - val_loss: 1.5046 - val_sparse_categorical_accuracy: 0.4279\n",
      "Epoch 2/20\n",
      "108/108 [==============================] - 33s 305ms/step - loss: 1.3523 - sparse_categorical_accuracy: 0.4895 - val_loss: 1.3443 - val_sparse_categorical_accuracy: 0.4836\n",
      "Epoch 3/20\n",
      "108/108 [==============================] - 39s 360ms/step - loss: 1.1245 - sparse_categorical_accuracy: 0.5918 - val_loss: 1.2351 - val_sparse_categorical_accuracy: 0.5313\n",
      "Epoch 4/20\n",
      "108/108 [==============================] - 63s 584ms/step - loss: 0.9499 - sparse_categorical_accuracy: 0.6553 - val_loss: 1.0616 - val_sparse_categorical_accuracy: 0.6015\n",
      "Epoch 5/20\n",
      "108/108 [==============================] - 52s 481ms/step - loss: 0.8097 - sparse_categorical_accuracy: 0.7046 - val_loss: 1.1271 - val_sparse_categorical_accuracy: 0.5936\n",
      "Epoch 6/20\n",
      "108/108 [==============================] - 39s 364ms/step - loss: 0.7187 - sparse_categorical_accuracy: 0.7394 - val_loss: 1.0057 - val_sparse_categorical_accuracy: 0.6303\n",
      "Epoch 7/20\n",
      "108/108 [==============================] - 43s 400ms/step - loss: 0.6236 - sparse_categorical_accuracy: 0.7721 - val_loss: 0.9603 - val_sparse_categorical_accuracy: 0.6472\n",
      "Epoch 8/20\n",
      "108/108 [==============================] - 44s 411ms/step - loss: 0.5581 - sparse_categorical_accuracy: 0.7980 - val_loss: 1.2342 - val_sparse_categorical_accuracy: 0.5555\n",
      "Epoch 9/20\n",
      "108/108 [==============================] - 39s 364ms/step - loss: 0.5054 - sparse_categorical_accuracy: 0.8179 - val_loss: 1.0519 - val_sparse_categorical_accuracy: 0.6282\n",
      "Epoch 10/20\n",
      "108/108 [==============================] - 47s 438ms/step - loss: 0.4555 - sparse_categorical_accuracy: 0.8339 - val_loss: 1.0877 - val_sparse_categorical_accuracy: 0.6512\n",
      "Epoch 11/20\n",
      "108/108 [==============================] - 52s 483ms/step - loss: 0.4117 - sparse_categorical_accuracy: 0.8510 - val_loss: 0.9938 - val_sparse_categorical_accuracy: 0.6777\n",
      "Epoch 12/20\n",
      "108/108 [==============================] - 54s 501ms/step - loss: 0.3854 - sparse_categorical_accuracy: 0.8571 - val_loss: 1.0502 - val_sparse_categorical_accuracy: 0.6719\n",
      "Epoch 13/20\n",
      "108/108 [==============================] - 55s 505ms/step - loss: 0.3569 - sparse_categorical_accuracy: 0.8698 - val_loss: 1.0568 - val_sparse_categorical_accuracy: 0.6798\n",
      "Epoch 14/20\n",
      "108/108 [==============================] - 54s 501ms/step - loss: 0.3306 - sparse_categorical_accuracy: 0.8768 - val_loss: 1.2551 - val_sparse_categorical_accuracy: 0.6518\n",
      "Epoch 15/20\n",
      "108/108 [==============================] - 54s 501ms/step - loss: 0.3112 - sparse_categorical_accuracy: 0.8871 - val_loss: 1.1873 - val_sparse_categorical_accuracy: 0.6754\n",
      "Epoch 16/20\n",
      "108/108 [==============================] - 54s 497ms/step - loss: 0.2987 - sparse_categorical_accuracy: 0.8931 - val_loss: 1.2479 - val_sparse_categorical_accuracy: 0.6501\n",
      "Epoch 17/20\n",
      "108/108 [==============================] - 54s 504ms/step - loss: 0.2839 - sparse_categorical_accuracy: 0.8984 - val_loss: 2.0406 - val_sparse_categorical_accuracy: 0.5916\n",
      "Epoch 18/20\n",
      "108/108 [==============================] - 53s 495ms/step - loss: 0.2665 - sparse_categorical_accuracy: 0.9038 - val_loss: 1.2636 - val_sparse_categorical_accuracy: 0.6614\n",
      "Epoch 19/20\n",
      "108/108 [==============================] - 53s 493ms/step - loss: 0.2519 - sparse_categorical_accuracy: 0.9067 - val_loss: 1.3850 - val_sparse_categorical_accuracy: 0.6640\n",
      "Epoch 20/20\n",
      "108/108 [==============================] - 44s 406ms/step - loss: 0.2464 - sparse_categorical_accuracy: 0.9103 - val_loss: 1.2977 - val_sparse_categorical_accuracy: 0.6667\n",
      "134/134 [==============================] - 5s 35ms/step\n",
      "\n",
      "accuracy:  0.7437033582089553\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_epochs = 20\n",
    "\n",
    "model_em1 = models.Sequential()\n",
    "model_em1.add(layers.Embedding(\n",
    "    vocab_size,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False\n",
    "))\n",
    "model_em1.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model_em1.add(layers.MaxPooling1D(5))\n",
    "model_em1.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model_em1.add(layers.MaxPooling1D(5))\n",
    "model_em1.add(layers.GlobalMaxPooling1D())\n",
    "model_em1.add(layers.Dropout(0.5))\n",
    "model_em1.add(layers.Dense(128, activation=\"relu\"))\n",
    "model_em1.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model_em1.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='rmsprop',\n",
    "    metrics=['sparse_categorical_accuracy']\n",
    ")\n",
    " \n",
    "model_em1.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=num_epochs,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "pred_em1 = model_em1.predict(x_test)\n",
    "pred_em1 = np.argmax(pred_em1, axis=1)\n",
    "\n",
    "print('\\naccuracy: ', accuracy_score(y_test, pred_em1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is a clear improvement over the first CNN. This can potentially be attributed to both the more complex network topology as well as the use of the pretrained embeddings from GloVe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now see if using one of GloVe's higher dimension embeddings can improve our results. We will use the 300 dimension version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "\n",
    "embeddings_index = {}\n",
    "with open('data/glove.6B/glove.6B.300d.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "108/108 [==============================] - 89s 800ms/step - loss: 1.5294 - sparse_categorical_accuracy: 0.3947 - val_loss: 1.3807 - val_sparse_categorical_accuracy: 0.4655\n",
      "Epoch 2/20\n",
      "108/108 [==============================] - 95s 878ms/step - loss: 1.1716 - sparse_categorical_accuracy: 0.5610 - val_loss: 1.1656 - val_sparse_categorical_accuracy: 0.5636\n",
      "Epoch 3/20\n",
      "108/108 [==============================] - 102s 941ms/step - loss: 0.8727 - sparse_categorical_accuracy: 0.6883 - val_loss: 1.0776 - val_sparse_categorical_accuracy: 0.6224\n",
      "Epoch 4/20\n",
      "108/108 [==============================] - 102s 945ms/step - loss: 0.6758 - sparse_categorical_accuracy: 0.7606 - val_loss: 0.9550 - val_sparse_categorical_accuracy: 0.6594\n",
      "Epoch 5/20\n",
      "108/108 [==============================] - 92s 853ms/step - loss: 0.5409 - sparse_categorical_accuracy: 0.8063 - val_loss: 0.8576 - val_sparse_categorical_accuracy: 0.7112\n",
      "Epoch 6/20\n",
      "108/108 [==============================] - 84s 782ms/step - loss: 0.4466 - sparse_categorical_accuracy: 0.8447 - val_loss: 1.2581 - val_sparse_categorical_accuracy: 0.6475\n",
      "Epoch 7/20\n",
      "108/108 [==============================] - 90s 837ms/step - loss: 0.3835 - sparse_categorical_accuracy: 0.8643 - val_loss: 0.8679 - val_sparse_categorical_accuracy: 0.7083\n",
      "Epoch 8/20\n",
      "108/108 [==============================] - 88s 819ms/step - loss: 0.3299 - sparse_categorical_accuracy: 0.8836 - val_loss: 0.9771 - val_sparse_categorical_accuracy: 0.6993\n",
      "Epoch 9/20\n",
      "108/108 [==============================] - 93s 863ms/step - loss: 0.2952 - sparse_categorical_accuracy: 0.8929 - val_loss: 1.4610 - val_sparse_categorical_accuracy: 0.6565\n",
      "Epoch 10/20\n",
      "108/108 [==============================] - 97s 893ms/step - loss: 0.2805 - sparse_categorical_accuracy: 0.9001 - val_loss: 1.0690 - val_sparse_categorical_accuracy: 0.7106\n",
      "Epoch 11/20\n",
      "108/108 [==============================] - 95s 877ms/step - loss: 0.2420 - sparse_categorical_accuracy: 0.9150 - val_loss: 1.1444 - val_sparse_categorical_accuracy: 0.7057\n",
      "Epoch 12/20\n",
      "108/108 [==============================] - 92s 856ms/step - loss: 0.2236 - sparse_categorical_accuracy: 0.9185 - val_loss: 1.5131 - val_sparse_categorical_accuracy: 0.6873\n",
      "Epoch 13/20\n",
      "108/108 [==============================] - 92s 849ms/step - loss: 0.2054 - sparse_categorical_accuracy: 0.9261 - val_loss: 1.2661 - val_sparse_categorical_accuracy: 0.6932\n",
      "Epoch 14/20\n",
      "108/108 [==============================] - 93s 858ms/step - loss: 0.2096 - sparse_categorical_accuracy: 0.9275 - val_loss: 1.3126 - val_sparse_categorical_accuracy: 0.7028\n",
      "Epoch 15/20\n",
      "108/108 [==============================] - 92s 849ms/step - loss: 0.1756 - sparse_categorical_accuracy: 0.9388 - val_loss: 1.7706 - val_sparse_categorical_accuracy: 0.6771\n",
      "Epoch 16/20\n",
      "108/108 [==============================] - 92s 848ms/step - loss: 0.1772 - sparse_categorical_accuracy: 0.9365 - val_loss: 1.4497 - val_sparse_categorical_accuracy: 0.6873\n",
      "Epoch 17/20\n",
      "108/108 [==============================] - 93s 858ms/step - loss: 0.1661 - sparse_categorical_accuracy: 0.9423 - val_loss: 1.4122 - val_sparse_categorical_accuracy: 0.6908\n",
      "Epoch 18/20\n",
      "108/108 [==============================] - 89s 823ms/step - loss: 0.1610 - sparse_categorical_accuracy: 0.9429 - val_loss: 1.5872 - val_sparse_categorical_accuracy: 0.6914\n",
      "Epoch 19/20\n",
      "108/108 [==============================] - 94s 869ms/step - loss: 0.1595 - sparse_categorical_accuracy: 0.9438 - val_loss: 1.7140 - val_sparse_categorical_accuracy: 0.6862\n",
      "Epoch 20/20\n",
      "108/108 [==============================] - 92s 852ms/step - loss: 0.1462 - sparse_categorical_accuracy: 0.9481 - val_loss: 1.6737 - val_sparse_categorical_accuracy: 0.6902\n",
      "134/134 [==============================] - 12s 89ms/step\n",
      "\n",
      "accuracy:  0.7541977611940298\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_epochs = 20\n",
    "\n",
    "model_em2 = models.Sequential()\n",
    "model_em2.add(layers.Embedding(\n",
    "    vocab_size,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False\n",
    "))\n",
    "model_em2.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model_em2.add(layers.MaxPooling1D(5))\n",
    "model_em2.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model_em2.add(layers.MaxPooling1D(5))\n",
    "model_em2.add(layers.GlobalMaxPooling1D())\n",
    "model_em2.add(layers.Dropout(0.5))\n",
    "model_em2.add(layers.Dense(128, activation=\"relu\"))\n",
    "model_em2.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model_em2.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='rmsprop',\n",
    "    metrics=['sparse_categorical_accuracy']\n",
    ")\n",
    " \n",
    "model_em2.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=num_epochs,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "pred_em2 = model_em2.predict(x_test)\n",
    "pred_em2 = np.argmax(pred_em2, axis=1)\n",
    "\n",
    "print('\\naccuracy: ', accuracy_score(y_test, pred_em2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy has slightly improved over the previous version. However, the accuracy is still less than that of the dense sequential model, indicating that the GloVe embeddings and/or CNNs are not very useful for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now see if the GloVe embeddings can be used with a dense sequential network to get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "108/108 [==============================] - 8s 59ms/step - loss: 1.5869 - sparse_categorical_accuracy: 0.3439 - val_loss: 1.6135 - val_sparse_categorical_accuracy: 0.2958\n",
      "Epoch 2/30\n",
      "108/108 [==============================] - 6s 60ms/step - loss: 1.5552 - sparse_categorical_accuracy: 0.3788 - val_loss: 1.6155 - val_sparse_categorical_accuracy: 0.2993\n",
      "Epoch 3/30\n",
      "108/108 [==============================] - 7s 61ms/step - loss: 1.4894 - sparse_categorical_accuracy: 0.4512 - val_loss: 1.5167 - val_sparse_categorical_accuracy: 0.3991\n",
      "Epoch 4/30\n",
      "108/108 [==============================] - 7s 68ms/step - loss: 1.4104 - sparse_categorical_accuracy: 0.4692 - val_loss: 1.5461 - val_sparse_categorical_accuracy: 0.3857\n",
      "Epoch 5/30\n",
      "108/108 [==============================] - 7s 68ms/step - loss: 1.3572 - sparse_categorical_accuracy: 0.4837 - val_loss: 1.4351 - val_sparse_categorical_accuracy: 0.4521\n",
      "Epoch 6/30\n",
      "108/108 [==============================] - 7s 67ms/step - loss: 1.3231 - sparse_categorical_accuracy: 0.4987 - val_loss: 1.3937 - val_sparse_categorical_accuracy: 0.4658\n",
      "Epoch 7/30\n",
      "108/108 [==============================] - 7s 65ms/step - loss: 1.2978 - sparse_categorical_accuracy: 0.5085 - val_loss: 1.4261 - val_sparse_categorical_accuracy: 0.4480\n",
      "Epoch 8/30\n",
      "108/108 [==============================] - 7s 67ms/step - loss: 1.2763 - sparse_categorical_accuracy: 0.5244 - val_loss: 1.4770 - val_sparse_categorical_accuracy: 0.4099\n",
      "Epoch 9/30\n",
      "108/108 [==============================] - 8s 70ms/step - loss: 1.2665 - sparse_categorical_accuracy: 0.5251 - val_loss: 1.3479 - val_sparse_categorical_accuracy: 0.4969\n",
      "Epoch 10/30\n",
      "108/108 [==============================] - 7s 67ms/step - loss: 1.2513 - sparse_categorical_accuracy: 0.5358 - val_loss: 1.3416 - val_sparse_categorical_accuracy: 0.5022\n",
      "Epoch 11/30\n",
      "108/108 [==============================] - 7s 64ms/step - loss: 1.2333 - sparse_categorical_accuracy: 0.5426 - val_loss: 1.3334 - val_sparse_categorical_accuracy: 0.5063\n",
      "Epoch 12/30\n",
      "108/108 [==============================] - 7s 67ms/step - loss: 1.2093 - sparse_categorical_accuracy: 0.5550 - val_loss: 1.4417 - val_sparse_categorical_accuracy: 0.4591\n",
      "Epoch 13/30\n",
      "108/108 [==============================] - 7s 69ms/step - loss: 1.1941 - sparse_categorical_accuracy: 0.5558 - val_loss: 1.3334 - val_sparse_categorical_accuracy: 0.5007\n",
      "Epoch 14/30\n",
      "108/108 [==============================] - 7s 68ms/step - loss: 1.1750 - sparse_categorical_accuracy: 0.5587 - val_loss: 1.3311 - val_sparse_categorical_accuracy: 0.5089\n",
      "Epoch 15/30\n",
      "108/108 [==============================] - 7s 69ms/step - loss: 1.1573 - sparse_categorical_accuracy: 0.5644 - val_loss: 1.2850 - val_sparse_categorical_accuracy: 0.5130\n",
      "Epoch 16/30\n",
      "108/108 [==============================] - 7s 70ms/step - loss: 1.1382 - sparse_categorical_accuracy: 0.5736 - val_loss: 1.3782 - val_sparse_categorical_accuracy: 0.4585\n",
      "Epoch 17/30\n",
      "108/108 [==============================] - 7s 68ms/step - loss: 1.1304 - sparse_categorical_accuracy: 0.5733 - val_loss: 1.3432 - val_sparse_categorical_accuracy: 0.4812\n",
      "Epoch 18/30\n",
      "108/108 [==============================] - 7s 69ms/step - loss: 1.1168 - sparse_categorical_accuracy: 0.5817 - val_loss: 1.3470 - val_sparse_categorical_accuracy: 0.4771\n",
      "Epoch 19/30\n",
      "108/108 [==============================] - 8s 70ms/step - loss: 1.1038 - sparse_categorical_accuracy: 0.5864 - val_loss: 1.2672 - val_sparse_categorical_accuracy: 0.5092\n",
      "Epoch 20/30\n",
      "108/108 [==============================] - 7s 69ms/step - loss: 1.0946 - sparse_categorical_accuracy: 0.5874 - val_loss: 1.2726 - val_sparse_categorical_accuracy: 0.5103\n",
      "Epoch 21/30\n",
      "108/108 [==============================] - 7s 68ms/step - loss: 1.0833 - sparse_categorical_accuracy: 0.5908 - val_loss: 1.3228 - val_sparse_categorical_accuracy: 0.4929\n",
      "Epoch 22/30\n",
      "108/108 [==============================] - 7s 69ms/step - loss: 1.0788 - sparse_categorical_accuracy: 0.5936 - val_loss: 1.2533 - val_sparse_categorical_accuracy: 0.5211\n",
      "Epoch 23/30\n",
      "108/108 [==============================] - 7s 67ms/step - loss: 1.0730 - sparse_categorical_accuracy: 0.5976 - val_loss: 1.2633 - val_sparse_categorical_accuracy: 0.5191\n",
      "Epoch 24/30\n",
      "108/108 [==============================] - 7s 69ms/step - loss: 1.0613 - sparse_categorical_accuracy: 0.5982 - val_loss: 1.2588 - val_sparse_categorical_accuracy: 0.5138\n",
      "Epoch 25/30\n",
      "108/108 [==============================] - 7s 67ms/step - loss: 1.0544 - sparse_categorical_accuracy: 0.6037 - val_loss: 1.3446 - val_sparse_categorical_accuracy: 0.4608\n",
      "Epoch 26/30\n",
      "108/108 [==============================] - 7s 68ms/step - loss: 1.0496 - sparse_categorical_accuracy: 0.6029 - val_loss: 1.2331 - val_sparse_categorical_accuracy: 0.5226\n",
      "Epoch 27/30\n",
      "108/108 [==============================] - 7s 68ms/step - loss: 1.0461 - sparse_categorical_accuracy: 0.6069 - val_loss: 1.3440 - val_sparse_categorical_accuracy: 0.5045\n",
      "Epoch 28/30\n",
      "108/108 [==============================] - 7s 67ms/step - loss: 1.0393 - sparse_categorical_accuracy: 0.6077 - val_loss: 1.2683 - val_sparse_categorical_accuracy: 0.5095\n",
      "Epoch 29/30\n",
      "108/108 [==============================] - 7s 68ms/step - loss: 1.0320 - sparse_categorical_accuracy: 0.6089 - val_loss: 1.2146 - val_sparse_categorical_accuracy: 0.5380\n",
      "Epoch 30/30\n",
      "108/108 [==============================] - 8s 70ms/step - loss: 1.0253 - sparse_categorical_accuracy: 0.6150 - val_loss: 1.4162 - val_sparse_categorical_accuracy: 0.4926\n",
      "134/134 [==============================] - 3s 18ms/step\n",
      "\n",
      "accuracy:  0.5396455223880597\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_epochs = 30\n",
    "\n",
    "model_em3 = models.Sequential()\n",
    "model_em3.add(layers.Embedding(\n",
    "    vocab_size,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False\n",
    "))\n",
    "model_em3.add(layers.GlobalMaxPooling1D())\n",
    "model_em3.add(layers.Dense(32, kernel_initializer='normal', activation='relu'))\n",
    "model_em3.add(layers.Dense(32, kernel_initializer='normal', activation='relu'))\n",
    "model_em3.add(layers.Dense(num_classes, kernel_initializer='normal', activation='softmax'))\n",
    "\n",
    "model_em3.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='rmsprop',\n",
    "    metrics=['sparse_categorical_accuracy']\n",
    ")\n",
    " \n",
    "model_em3.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=num_epochs,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "pred_em3 = model_em3.predict(x_test)\n",
    "pred_em3 = np.argmax(pred_em3, axis=1)\n",
    "\n",
    "print('\\naccuracy: ', accuracy_score(y_test, pred_em3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is unfortunately the worst so far. It suggests that using an embedding without a CNN or RNN does not really add value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will try to use vectorizations as a basis for our embeddings rather than tokenizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "108/108 [==============================] - 27s 227ms/step - loss: 1.5891 - sparse_categorical_accuracy: 0.3292 - val_loss: 1.6143 - val_sparse_categorical_accuracy: 0.3162\n",
      "Epoch 2/20\n",
      "108/108 [==============================] - 21s 196ms/step - loss: 1.4435 - sparse_categorical_accuracy: 0.3893 - val_loss: 1.2660 - val_sparse_categorical_accuracy: 0.4969\n",
      "Epoch 3/20\n",
      "108/108 [==============================] - 21s 197ms/step - loss: 0.8360 - sparse_categorical_accuracy: 0.6765 - val_loss: 1.0228 - val_sparse_categorical_accuracy: 0.5991\n",
      "Epoch 4/20\n",
      "108/108 [==============================] - 22s 206ms/step - loss: 0.4476 - sparse_categorical_accuracy: 0.8310 - val_loss: 0.8150 - val_sparse_categorical_accuracy: 0.7051\n",
      "Epoch 5/20\n",
      "108/108 [==============================] - 22s 201ms/step - loss: 0.2415 - sparse_categorical_accuracy: 0.9208 - val_loss: 0.8044 - val_sparse_categorical_accuracy: 0.7205\n",
      "Epoch 6/20\n",
      "108/108 [==============================] - 21s 198ms/step - loss: 0.1758 - sparse_categorical_accuracy: 0.9439 - val_loss: 0.8403 - val_sparse_categorical_accuracy: 0.7397\n",
      "Epoch 7/20\n",
      "108/108 [==============================] - 22s 200ms/step - loss: 0.1272 - sparse_categorical_accuracy: 0.9570 - val_loss: 0.8109 - val_sparse_categorical_accuracy: 0.7354\n",
      "Epoch 8/20\n",
      "108/108 [==============================] - 22s 200ms/step - loss: 0.0999 - sparse_categorical_accuracy: 0.9672 - val_loss: 0.8779 - val_sparse_categorical_accuracy: 0.7377\n",
      "Epoch 9/20\n",
      "108/108 [==============================] - 24s 223ms/step - loss: 0.0755 - sparse_categorical_accuracy: 0.9750 - val_loss: 0.9467 - val_sparse_categorical_accuracy: 0.7397\n",
      "Epoch 10/20\n",
      "108/108 [==============================] - 26s 242ms/step - loss: 0.0604 - sparse_categorical_accuracy: 0.9804 - val_loss: 0.9529 - val_sparse_categorical_accuracy: 0.7354\n",
      "Epoch 11/20\n",
      "108/108 [==============================] - 45s 417ms/step - loss: 0.0494 - sparse_categorical_accuracy: 0.9848 - val_loss: 0.9872 - val_sparse_categorical_accuracy: 0.7389\n",
      "Epoch 12/20\n",
      "108/108 [==============================] - 55s 508ms/step - loss: 0.0401 - sparse_categorical_accuracy: 0.9868 - val_loss: 1.0320 - val_sparse_categorical_accuracy: 0.7377\n",
      "Epoch 13/20\n",
      "108/108 [==============================] - 23s 210ms/step - loss: 0.0349 - sparse_categorical_accuracy: 0.9890 - val_loss: 1.0991 - val_sparse_categorical_accuracy: 0.7400\n",
      "Epoch 14/20\n",
      "108/108 [==============================] - 20s 183ms/step - loss: 0.0328 - sparse_categorical_accuracy: 0.9902 - val_loss: 1.1438 - val_sparse_categorical_accuracy: 0.7319\n",
      "Epoch 15/20\n",
      "108/108 [==============================] - 22s 206ms/step - loss: 0.0270 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.1041 - val_sparse_categorical_accuracy: 0.7394\n",
      "Epoch 16/20\n",
      "108/108 [==============================] - 36s 334ms/step - loss: 0.0254 - sparse_categorical_accuracy: 0.9921 - val_loss: 1.1492 - val_sparse_categorical_accuracy: 0.7365\n",
      "Epoch 17/20\n",
      "108/108 [==============================] - 40s 368ms/step - loss: 0.0204 - sparse_categorical_accuracy: 0.9936 - val_loss: 1.1962 - val_sparse_categorical_accuracy: 0.7397\n",
      "Epoch 18/20\n",
      "108/108 [==============================] - 30s 279ms/step - loss: 0.0193 - sparse_categorical_accuracy: 0.9939 - val_loss: 1.2036 - val_sparse_categorical_accuracy: 0.7383\n",
      "Epoch 19/20\n",
      "108/108 [==============================] - 27s 254ms/step - loss: 0.0182 - sparse_categorical_accuracy: 0.9935 - val_loss: 1.2196 - val_sparse_categorical_accuracy: 0.7336\n",
      "Epoch 20/20\n",
      "108/108 [==============================] - 29s 265ms/step - loss: 0.0187 - sparse_categorical_accuracy: 0.9934 - val_loss: 1.2705 - val_sparse_categorical_accuracy: 0.7354\n",
      "134/134 [==============================] - 3s 22ms/step\n",
      "\n",
      "accuracy:  0.8708022388059702\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "max_len = 200\n",
    "batch_size = 128\n",
    "num_epochs = 20\n",
    "embedding_dim = 128\n",
    "\n",
    "vectorizer = TextVectorization(max_tokens=vocab_size, output_sequence_length=max_len)\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(train.Text).batch(batch_size)\n",
    "vectorizer.adapt(text_ds)\n",
    "\n",
    "voc = vectorizer.get_vocabulary()\n",
    "word_index = dict(zip(voc, range(len(voc))))\n",
    "\n",
    "x_train = vectorizer(np.array([[s] for s in train.Text])).numpy()\n",
    "x_test = vectorizer(np.array([[s] for s in test.Text])).numpy()\n",
    "\n",
    "model_em4 = models.Sequential()\n",
    "model_em4.add(layers.Embedding(len(word_index) + 1, embedding_dim, input_length=max_len))\n",
    "model_em4.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model_em4.add(layers.MaxPooling1D(5))\n",
    "model_em4.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model_em4.add(layers.MaxPooling1D(5))\n",
    "model_em4.add(layers.GlobalMaxPooling1D())\n",
    "model_em4.add(layers.Dropout(0.5))\n",
    "model_em4.add(layers.Dense(128, activation=\"relu\"))\n",
    "model_em4.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model_em4.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='rmsprop',\n",
    "    metrics=['sparse_categorical_accuracy']\n",
    ")\n",
    " \n",
    "model_em4.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=num_epochs,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "pred_em4 = model_em4.predict(x_test)\n",
    "pred_em4 = np.argmax(pred_em4, axis=1)\n",
    "\n",
    "print('\\naccuracy: ', accuracy_score(y_test, pred_em4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model slightly outperformed our original model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "The best models were the dense sequential network and the last CNN, which utilized vectorizations of the input data. The sequential network likely outperformed most of the CNNs because looking for the presence or absence of words already gives a solid foundation for judging the sentiment of a text.\n",
    "\n",
    "However, the CNNs may in general have been able to perform better if they were given more training time. For all but the last CNN tested, the increase in accuracy between epochs had yet to significantly plateau. Thus, it is likely that all the CNNs in this notebook are not performing at their most optimal level. The number of epochs was cut down in the interest of time, but it is clear that CNNs simply take a longer time to achieve better results.\n",
    "\n",
    "Using pretrained embeddings definitely provides some value. Switching from the 100 dimension to 300 dimension versions of GloVe slightly improved performance, and as stated previously, all the CNNs may perform better if given more epochs to train. The true value of the GloVe embeddings may also be more apparent if RNNs were used on this dataset. However, RNNs were avoided simply because the training time is too long."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
